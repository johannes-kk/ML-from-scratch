{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed, randrange\n",
    "from csv import reader\n",
    "\n",
    "# Importing our own functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "Helper functions to easily fit, evaluate, and compare algorithms.\n",
    "* Train/test split evaluation harness\n",
    "* Cross-validation split evaluation harness\n",
    "* Dynamic evaluation harness with flexible choices of splits and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm is the algorithm to test, in our testing we use baseline algorithms\n",
    "# *args is for any additional configuration parameters required by the above algorithm\n",
    "# Requires train_test_split and accuracy_metric to be defined\n",
    "def split_evaluate_algorithm(dataset, algorithm, split, *args):\n",
    "    train, test = train_test_split(dataset, split)\n",
    "    test_set = list()\n",
    "    for row in test:\n",
    "        row_copy = list(row)\n",
    "        # Assuming last column is output, clear outputs to avoid accidental cheating by baseline prediction\n",
    "        row_copy[-1] = None\n",
    "        test_set.append(row_copy)\n",
    "    # Get baseline prediction using specified algorithm\n",
    "    predicted = algorithm(train, test_set, *args)\n",
    "    actual = [row[-1] for row in test]\n",
    "    # Assumes it's a classification problem, should make accuracy algorithm a parameter\n",
    "    accuracy = accuracy_metric(actual, predicted)\n",
    "    return accuracy\n",
    "\n",
    "def cross_evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    # List to store accuracy scores of each fold k used as test set\n",
    "    scores = list()\n",
    "    # For each fold:\n",
    "    for fold in folds:\n",
    "        # Use all folds except the current as training data\n",
    "        train_set = list(folds) # At this point, a list of folds where each fold is also a list\n",
    "        # Remove the current fold from training set, instead using it as the test set\n",
    "        train_set.remove(fold)\n",
    "        # Flattens the list of lists into one big list containing all the rows (= appends folds together)\n",
    "        train_set = sum(train_set, [])\n",
    "        # Initialize empty list to hold test data\n",
    "        test_set = list()\n",
    "        # Set current fold as test set, and nullify the output to avoid accidental cheating\n",
    "        for row in fold:\n",
    "            # row_copy = row   would reference the same object, instead of copying it\n",
    "            row_copy = list(row)\n",
    "            row_copy[-1] = None\n",
    "            test_set.append(row_copy)\n",
    "        # Predict based on training with all other folds than the current fold, and specified algorithm\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        # Set current fold outputs as test set\n",
    "        actual = [row[-1] for row in fold]\n",
    "        # Rate accuracy by comparing prediction to test set (i.e. current fold)\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# folds_or_split is number of folds or train/test split. Train/test used if <= 1.0, otherwise cross-validation\n",
    "# default metric to evaluate is accuracy, hence assumes classification unless specified otherwise\n",
    "# algorithm is the ML algorithm to test\n",
    "# Metric accuracy indirectly determines whether it's a classification or regression algorithm\n",
    "def evaluate_algorithm(dataset, algorithm, folds_or_split = 0.6, metric = accuracy_metric, *args):\n",
    "    scores = list()\n",
    "    folds = list()\n",
    "\n",
    "    if folds_or_split <= 1.0:\n",
    "        # Then train/test split\n",
    "        train, test = train_test_split(dataset, folds_or_split)\n",
    "        # Append test set first, since that will be the first to be iterated over\n",
    "        folds.append(test)\n",
    "        folds.append(train)\n",
    "    else:\n",
    "        # Then cross-validation\n",
    "        folds = cross_validation_split(dataset, folds_or_split)\n",
    "\n",
    "    for fold in folds:\n",
    "        # Training set is all folds but the current\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)  # Remove current fold, sine that is the test set\n",
    "        train_set = sum(train_set, [])  # Flatten training data into one set/list\n",
    "\n",
    "        # Nullify output values in test set to avoid accidental cheating\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            row_copy[-1] = None\n",
    "            test_set.append(row_copy)\n",
    "\n",
    "        # Predict values of test set based on specified algorithm\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        # Observed (real) data\n",
    "        actual = [row[-1] for row in fold]\n",
    "\n",
    "        # Calculate accuracy based on specified metric; depends on if classification or regression\n",
    "        accuracy = metric(actual, predicted)\n",
    "\n",
    "        # If train/test split, stop after first iteration since only one fold (i.e. the test set)\n",
    "        if folds_or_split <= 1.0:\n",
    "            # Return accuracy and exit function - breaking loop before reverse combination of \"folds\" is used\n",
    "            return accuracy\n",
    "        # If not, continue the iteration\n",
    "        else:\n",
    "            # Store score of this fold: will error if metric returns more than one value!\n",
    "            scores.append(accuracy)\n",
    "            continue\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing train/test harness on zero rule baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file data/pima-indians-diabetes.csv with 768 rows and 9 columns.\n",
      "Accuracy: 67.427%\n"
     ]
    }
   ],
   "source": [
    "# Testing train-test test harness\n",
    "seed(1)\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "# Convert all columns to floats\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# evaluate algorithm\n",
    "split = 0.6\n",
    "accuracy = split_evaluate_algorithm(dataset, zero_rule_algorithm_classification, split)\n",
    "print('Accuracy: %.3f%%' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing cross validation harness on zero rule baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data file data/pima-indians-diabetes.csv with 768 rows and 9 columns.\n",
      "Scores: [62.091503267973856, 64.70588235294117, 64.70588235294117, 64.70588235294117, 69.28104575163398]\n",
      "Mean Accuracy: 65.098%\n"
     ]
    }
   ],
   "source": [
    "# Testing cross-validation split harness\n",
    "seed(1)\n",
    "print('')\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "# Convert all columns to floats\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# evaluate algorithm\n",
    "folds = 5\n",
    "scores = cross_evaluate_algorithm(dataset, zero_rule_algorithm_classification, folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dynamic evaluation harness on zero rule baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data file data/pima-indians-diabetes.csv with 768 rows and 9 columns.\n",
      "[0.6156987634551992, 0.5940885257860046, 0.5940885257860046, 0.5940885257860046, 0.5542468245138262]\n",
      "0.5904422330654079\n",
      "Scores: [0.6156987634551992, 0.5940885257860046, 0.5940885257860046, 0.5940885257860046, 0.5542468245138262]\n",
      "Mean Accuracy: 59.044%\n"
     ]
    }
   ],
   "source": [
    "# Testing dynamic evaluation method\n",
    "seed(1)\n",
    "print('')\n",
    "dataset = load_csv('data/pima-indians-diabetes.csv')\n",
    "# Convert all data in dataset to float\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# Test method using zero_rule; coincidence that this is a baseline algo, can test any algo!\n",
    "scores = evaluate_algorithm(dataset, zero_rule_algorithm_classification, 5, rmse_metric)\n",
    "print(scores)\n",
    "print(sum(scores)/len(scores))\n",
    "\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (100*sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
