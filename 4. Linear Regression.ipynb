{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed, randrange\n",
    "\n",
    "# Importing our own functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "* Mean, variance, and covariance\n",
    "* Simple linear regression\n",
    "* Multiple linear regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))\n",
    "\n",
    "# Calculate variance of a lsit of numbers, provided mean\n",
    "def variance(values, mean):\n",
    "    return sum([(x - mean)**2 for x in values])\n",
    "\n",
    "# Calculate covariance between x and y\n",
    "def covariance(x, x_mean, y, y_mean):\n",
    "    covar = 0.0\n",
    "    for i in range(len(x)):\n",
    "        covar += (x[i] - x_mean) * (y[i] - y_mean)\n",
    "    return covar\n",
    "\n",
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    # Assumes x is first column, y second - creating a list for each\n",
    "    x = [row[0] for row in dataset]\n",
    "    y = [row[1] for row in dataset]\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    # Calculate simple linear regression coefficients\n",
    "    b1 = covariance(x, x_mean, y, y_mean) / variance(x, x_mean)\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    # Return list of coefficients\n",
    "    return [b0, b1]\n",
    "\n",
    "# Simple linear regression, assuming train/test split\n",
    "def simple_linear_regression(train, test):\n",
    "    predictions = list()\n",
    "    # Calculate regression coefficients\n",
    "    b0, b1 = coefficients(train)\n",
    "    # Test data used only to count iterations; parameter can be removed\n",
    "    # Include anwyway since most other algos require both test and train sets\n",
    "    for row in test:\n",
    "        # For each observation, predict y^ given x and coefficients\n",
    "        yhat = b0 + b1 * row[0]\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n",
    "\n",
    "# Make a prediction with provided coefficients\n",
    "# Returns yhat, i.e. predicted y-value for the provided row\n",
    "def predict_linear(row, coefficients):\n",
    "    # First coefficient in is always the intercept B0 (\"bias\")\n",
    "    yhat = coefficients[0]\n",
    "    # Iterate over remaining columns in row, multiplying Xi * Bi\n",
    "    for i in range(len(row)-1):\n",
    "        # Sum for prediction\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat\n",
    "\n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "# Returns list of coefficients, with intercept at first index\n",
    "# Batch size b = 1 means stochastic gradient descent, b > 1 means (mini) batch gradient descent\n",
    "def coefficients_linear_sgd(train, l_rate, n_epoch, b = 1):\n",
    "    # Start with empty coefficients\n",
    "    coef = [0.0 for col in range(len(train[0]))]\n",
    "    # Iterate over epochs, performing SGD based on batch size for each\n",
    "    for epoch in range(n_epoch):\n",
    "        # Sum total error to track overall epoch performance\n",
    "        sum_error = 0\n",
    "        # Counter to track mini batch\n",
    "        i = 0\n",
    "        # Counter to track overall row, to capture mid-batch end of dataset\n",
    "        j = 0\n",
    "        # SUM(h - y) * Xi over coefficients i, to multiply full batch by learning rate\n",
    "        adjustments = [0 for col in range(len(train[0]))]\n",
    "        # Iterate over all rows of data\n",
    "        for row in train:\n",
    "            # Increment batch index and dataset counters\n",
    "            i += 1\n",
    "            j += 1\n",
    "            # Predict y for this row with latest coefficients\n",
    "            yhat = predict_linear(row, coef)\n",
    "            # Find prediction error, assuming actual y is final column; i.e. (h - y)\n",
    "            error = yhat - row[-1]\n",
    "            # Squaring for absolute error, to track epoch performance\n",
    "            sum_error += error**2\n",
    "            # Add intercept (bias) error, since has no Xi\n",
    "            adjustments[0] += error\n",
    "            # Iterate over all coefficients, except the first (i.e. intercept)\n",
    "            for k in range(len(row)-1):\n",
    "                # Add to batch adjustment, i.e. (h - y) * Xi\n",
    "                # row column j belongs to coefficient j + 1, since j = 1 is intercept\n",
    "                adjustments[k + 1] += error * row[k]\n",
    "            # Check whether this row is last in batch, or in dataset\n",
    "            if i == b or j == len(train):\n",
    "                # Iterate over all coefficients\n",
    "                for k in range(len(adjustments)):\n",
    "                    # Adjust coefficient based on batch sum gradient and learning rate\n",
    "                    # i = b except if at end of dataset and in mid-batch\n",
    "                    # \"Averaging\" adjustment of each row in mini batch\n",
    "                    coef[k] = coef[k] - (1/i) * l_rate * adjustments[k]\n",
    "                # Reset adjustments to 0 for next batch\n",
    "                adjustments = [0 for col in range(len(train[0]))]\n",
    "                # Reset batch counter\n",
    "                i = 0\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch+1, l_rate, sum_error))\n",
    "    # Return the final set of coefficients\n",
    "    return coef\n",
    "\n",
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "# Returns the list of predictions for each dataset row\n",
    "# Parameters: learning rate, number of epochs (iterations), mini-batch size b\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch, b_size = 1):\n",
    "    predictions = list()\n",
    "    # Get coefficients based on SGD, learning rate and epochs\n",
    "    coef = coefficients_linear_sgd(train, l_rate, n_epoch, b_size)\n",
    "    # Get predictions for provided dataset using estimated coefficients\n",
    "    for row in test:\n",
    "        yhat = predict_linear(row, coef)\n",
    "        predictions.append(yhat)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x stats: mean=3.000 variance=10.000\n",
      "y stats: mean=2.800 variance=8.800\n",
      "Covariance: 8.000\n",
      "\n",
      "Contrived dataset: [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
      "Coefficients: B0=0.400, B1=0.800\n",
      "Predictions: [1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
      "RMSE: 0.692820323027551\n",
      "\n",
      "Testing contrived dataset, using default 0.6 split:\n",
      "RMSE: 1.061\n",
      "Note: Expect higher RMSE since here trained on 3 observations and tested on 2\n",
      "\n",
      "Testing Swedish Auto Insurance:\n",
      "Loaded data file data/insurance.csv with 63 rows and 2 columns.\n",
      "RMSE: 41.321\n"
     ]
    }
   ],
   "source": [
    "# Test calculate mean, variance, and covariance\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "x = [row[0] for row in dataset]\n",
    "y = [row[1] for row in dataset]\n",
    "mean_x, mean_y = mean(x), mean(y)\n",
    "var_x, var_y = variance(x, mean_x), variance(y, mean_y)\n",
    "covar = covariance(x, mean_x, y, mean_y)\n",
    "print('x stats: mean=%.3f variance=%.3f' % (mean_x, var_x))\n",
    "print('y stats: mean=%.3f variance=%.3f' % (mean_y, var_y))\n",
    "print('Covariance: %.3f' % (covar))\n",
    "\n",
    "# Test calculating coefficients and finding predictions\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "# Generally, nullify test set to avoid accidental cheating\n",
    "# (Our simple linear regression only uses the test set to iterate over rows, however)\n",
    "test_set = list()\n",
    "for row in dataset:\n",
    "    row_copy = list(row)\n",
    "    row_copy[-1] = None\n",
    "    test_set.append(row_copy)\n",
    "    \n",
    "print('\\nContrived dataset:', dataset)\n",
    "b0, b1 = coefficients(dataset)\n",
    "print('Coefficients: B0=%.3f, B1=%.3f' % (b0, b1))\n",
    "predicted = simple_linear_regression(dataset, test_set)\n",
    "print('Predictions:', simple_linear_regression(dataset, test_set))\n",
    "actual = [row[-1] for row in dataset]\n",
    "print('RMSE:', rmse_metric(actual, predicted))\n",
    "\n",
    "# Testing simple linear regression and RMSE evaluation on contrived dataset\n",
    "print('\\nTesting contrived dataset, using default 0.6 split:')\n",
    "seed(1)\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression, 0.6, rmse_metric)\n",
    "print('RMSE: %.3f' % (rmse))\n",
    "print('Note: Expect higher RMSE since here trained on 3 observations and tested on 2')\n",
    "\n",
    "# Testing simple linear regression and RMSE evaluation on Swedish Auto Insurance\n",
    "print('\\nTesting Swedish Auto Insurance:', )\n",
    "# Load dataset and convert columns to floats\n",
    "dataset = load_csv('data/insurance.csv')\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# Run and evaluate simple linear regression algorithm using dynamic harness\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression, 0.6, rmse_metric)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=2.0, Predicted=1.8\n",
      "Expected=1.0, Predicted=3.8\n",
      "Expected=4.0, Predicted=5.4\n",
      "Expected=2.0, Predicted=4.0\n",
      "Expected=4.0, Predicted=7.4\n",
      "\n",
      ">epoch=1, lrate=0.001, error=38.455\n",
      ">epoch=2, lrate=0.001, error=31.701\n",
      ">epoch=3, lrate=0.001, error=26.254\n",
      ">epoch=4, lrate=0.001, error=21.861\n",
      ">epoch=5, lrate=0.001, error=18.318\n",
      ">epoch=6, lrate=0.001, error=15.460\n",
      ">epoch=7, lrate=0.001, error=13.154\n",
      ">epoch=8, lrate=0.001, error=11.294\n",
      ">epoch=9, lrate=0.001, error=9.793\n",
      ">epoch=10, lrate=0.001, error=8.582\n",
      ">epoch=11, lrate=0.001, error=7.604\n",
      ">epoch=12, lrate=0.001, error=6.815\n",
      ">epoch=13, lrate=0.001, error=6.177\n",
      ">epoch=14, lrate=0.001, error=5.662\n",
      ">epoch=15, lrate=0.001, error=5.246\n",
      ">epoch=16, lrate=0.001, error=4.910\n",
      ">epoch=17, lrate=0.001, error=4.637\n",
      ">epoch=18, lrate=0.001, error=4.417\n",
      ">epoch=19, lrate=0.001, error=4.238\n",
      ">epoch=20, lrate=0.001, error=4.093\n",
      ">epoch=21, lrate=0.001, error=3.976\n",
      ">epoch=22, lrate=0.001, error=3.880\n",
      ">epoch=23, lrate=0.001, error=3.802\n",
      ">epoch=24, lrate=0.001, error=3.738\n",
      ">epoch=25, lrate=0.001, error=3.686\n",
      ">epoch=26, lrate=0.001, error=3.642\n",
      ">epoch=27, lrate=0.001, error=3.607\n",
      ">epoch=28, lrate=0.001, error=3.578\n",
      ">epoch=29, lrate=0.001, error=3.553\n",
      ">epoch=30, lrate=0.001, error=3.532\n",
      ">epoch=31, lrate=0.001, error=3.515\n",
      ">epoch=32, lrate=0.001, error=3.500\n",
      ">epoch=33, lrate=0.001, error=3.487\n",
      ">epoch=34, lrate=0.001, error=3.476\n",
      ">epoch=35, lrate=0.001, error=3.467\n",
      ">epoch=36, lrate=0.001, error=3.458\n",
      ">epoch=37, lrate=0.001, error=3.450\n",
      ">epoch=38, lrate=0.001, error=3.443\n",
      ">epoch=39, lrate=0.001, error=3.437\n",
      ">epoch=40, lrate=0.001, error=3.431\n",
      ">epoch=41, lrate=0.001, error=3.425\n",
      ">epoch=42, lrate=0.001, error=3.420\n",
      ">epoch=43, lrate=0.001, error=3.415\n",
      ">epoch=44, lrate=0.001, error=3.410\n",
      ">epoch=45, lrate=0.001, error=3.406\n",
      ">epoch=46, lrate=0.001, error=3.401\n",
      ">epoch=47, lrate=0.001, error=3.397\n",
      ">epoch=48, lrate=0.001, error=3.393\n",
      ">epoch=49, lrate=0.001, error=3.389\n",
      ">epoch=50, lrate=0.001, error=3.385\n",
      ">epoch=51, lrate=0.001, error=3.381\n",
      ">epoch=52, lrate=0.001, error=3.377\n",
      ">epoch=53, lrate=0.001, error=3.373\n",
      ">epoch=54, lrate=0.001, error=3.369\n",
      ">epoch=55, lrate=0.001, error=3.365\n",
      ">epoch=56, lrate=0.001, error=3.361\n",
      ">epoch=57, lrate=0.001, error=3.357\n",
      ">epoch=58, lrate=0.001, error=3.353\n",
      ">epoch=59, lrate=0.001, error=3.350\n",
      ">epoch=60, lrate=0.001, error=3.346\n",
      ">epoch=61, lrate=0.001, error=3.342\n",
      ">epoch=62, lrate=0.001, error=3.338\n",
      ">epoch=63, lrate=0.001, error=3.335\n",
      ">epoch=64, lrate=0.001, error=3.331\n",
      ">epoch=65, lrate=0.001, error=3.327\n",
      ">epoch=66, lrate=0.001, error=3.324\n",
      ">epoch=67, lrate=0.001, error=3.320\n",
      ">epoch=68, lrate=0.001, error=3.316\n",
      ">epoch=69, lrate=0.001, error=3.313\n",
      ">epoch=70, lrate=0.001, error=3.309\n",
      ">epoch=71, lrate=0.001, error=3.305\n",
      ">epoch=72, lrate=0.001, error=3.302\n",
      ">epoch=73, lrate=0.001, error=3.298\n",
      ">epoch=74, lrate=0.001, error=3.295\n",
      ">epoch=75, lrate=0.001, error=3.291\n",
      ">epoch=76, lrate=0.001, error=3.287\n",
      ">epoch=77, lrate=0.001, error=3.284\n",
      ">epoch=78, lrate=0.001, error=3.280\n",
      ">epoch=79, lrate=0.001, error=3.277\n",
      ">epoch=80, lrate=0.001, error=3.273\n",
      ">epoch=81, lrate=0.001, error=3.270\n",
      ">epoch=82, lrate=0.001, error=3.266\n",
      ">epoch=83, lrate=0.001, error=3.263\n",
      ">epoch=84, lrate=0.001, error=3.259\n",
      ">epoch=85, lrate=0.001, error=3.256\n",
      ">epoch=86, lrate=0.001, error=3.252\n",
      ">epoch=87, lrate=0.001, error=3.249\n",
      ">epoch=88, lrate=0.001, error=3.245\n",
      ">epoch=89, lrate=0.001, error=3.242\n",
      ">epoch=90, lrate=0.001, error=3.238\n",
      ">epoch=91, lrate=0.001, error=3.235\n",
      ">epoch=92, lrate=0.001, error=3.232\n",
      ">epoch=93, lrate=0.001, error=3.228\n",
      ">epoch=94, lrate=0.001, error=3.225\n",
      ">epoch=95, lrate=0.001, error=3.221\n",
      ">epoch=96, lrate=0.001, error=3.218\n",
      ">epoch=97, lrate=0.001, error=3.215\n",
      ">epoch=98, lrate=0.001, error=3.211\n",
      ">epoch=99, lrate=0.001, error=3.208\n",
      ">epoch=100, lrate=0.001, error=3.205\n",
      "\n",
      "Intercept:     0.16168989767075748\n",
      "Coefficient 1: 0.5083303467598079\n",
      "Coefficient 2: 0.2960510622448182\n",
      "\n",
      "Linear Regression With Stochastic Gradient Descent for Wine Quality:\n",
      "Loaded data file data/winequality-white.csv with 4898 rows and 12 columns.\n",
      ">epoch=1, lrate=0.010, error=12745.670\n",
      ">epoch=2, lrate=0.010, error=2831.499\n",
      ">epoch=3, lrate=0.010, error=2773.872\n",
      ">epoch=4, lrate=0.010, error=2727.172\n",
      ">epoch=5, lrate=0.010, error=2688.649\n",
      ">epoch=6, lrate=0.010, error=2656.251\n",
      ">epoch=7, lrate=0.010, error=2628.512\n",
      ">epoch=8, lrate=0.010, error=2604.375\n",
      ">epoch=9, lrate=0.010, error=2583.073\n",
      ">epoch=10, lrate=0.010, error=2564.043\n",
      ">epoch=11, lrate=0.010, error=2546.868\n",
      ">epoch=12, lrate=0.010, error=2531.236\n",
      ">epoch=13, lrate=0.010, error=2516.908\n",
      ">epoch=14, lrate=0.010, error=2503.700\n",
      ">epoch=15, lrate=0.010, error=2491.467\n",
      ">epoch=16, lrate=0.010, error=2480.095\n",
      ">epoch=17, lrate=0.010, error=2469.488\n",
      ">epoch=18, lrate=0.010, error=2459.571\n",
      ">epoch=19, lrate=0.010, error=2450.276\n",
      ">epoch=20, lrate=0.010, error=2441.549\n",
      ">epoch=21, lrate=0.010, error=2433.342\n",
      ">epoch=22, lrate=0.010, error=2425.613\n",
      ">epoch=23, lrate=0.010, error=2418.326\n",
      ">epoch=24, lrate=0.010, error=2411.446\n",
      ">epoch=25, lrate=0.010, error=2404.946\n",
      ">epoch=26, lrate=0.010, error=2398.799\n",
      ">epoch=27, lrate=0.010, error=2392.979\n",
      ">epoch=28, lrate=0.010, error=2387.467\n",
      ">epoch=29, lrate=0.010, error=2382.242\n",
      ">epoch=30, lrate=0.010, error=2377.285\n",
      ">epoch=31, lrate=0.010, error=2372.580\n",
      ">epoch=32, lrate=0.010, error=2368.111\n",
      ">epoch=33, lrate=0.010, error=2363.864\n",
      ">epoch=34, lrate=0.010, error=2359.826\n",
      ">epoch=35, lrate=0.010, error=2355.984\n",
      ">epoch=36, lrate=0.010, error=2352.327\n",
      ">epoch=37, lrate=0.010, error=2348.844\n",
      ">epoch=38, lrate=0.010, error=2345.526\n",
      ">epoch=39, lrate=0.010, error=2342.363\n",
      ">epoch=40, lrate=0.010, error=2339.347\n",
      ">epoch=41, lrate=0.010, error=2336.469\n",
      ">epoch=42, lrate=0.010, error=2333.723\n",
      ">epoch=43, lrate=0.010, error=2331.101\n",
      ">epoch=44, lrate=0.010, error=2328.597\n",
      ">epoch=45, lrate=0.010, error=2326.204\n",
      ">epoch=46, lrate=0.010, error=2323.916\n",
      ">epoch=47, lrate=0.010, error=2321.729\n",
      ">epoch=48, lrate=0.010, error=2319.637\n",
      ">epoch=49, lrate=0.010, error=2317.635\n",
      ">epoch=50, lrate=0.010, error=2315.719\n",
      ">epoch=1, lrate=0.010, error=12975.744\n",
      ">epoch=2, lrate=0.010, error=2758.087\n",
      ">epoch=3, lrate=0.010, error=2700.083\n",
      ">epoch=4, lrate=0.010, error=2653.622\n",
      ">epoch=5, lrate=0.010, error=2615.520\n",
      ">epoch=6, lrate=0.010, error=2583.641\n",
      ">epoch=7, lrate=0.010, error=2556.463\n",
      ">epoch=8, lrate=0.010, error=2532.893\n",
      ">epoch=9, lrate=0.010, error=2512.145\n",
      ">epoch=10, lrate=0.010, error=2493.643\n",
      ">epoch=11, lrate=0.010, error=2476.966\n",
      ">epoch=12, lrate=0.010, error=2461.798\n",
      ">epoch=13, lrate=0.010, error=2447.901\n",
      ">epoch=14, lrate=0.010, error=2435.093\n",
      ">epoch=15, lrate=0.010, error=2423.231\n",
      ">epoch=16, lrate=0.010, error=2412.203\n",
      ">epoch=17, lrate=0.010, error=2401.916\n",
      ">epoch=18, lrate=0.010, error=2392.295\n",
      ">epoch=19, lrate=0.010, error=2383.278\n",
      ">epoch=20, lrate=0.010, error=2374.811\n",
      ">epoch=21, lrate=0.010, error=2366.848\n",
      ">epoch=22, lrate=0.010, error=2359.349\n",
      ">epoch=23, lrate=0.010, error=2352.277\n",
      ">epoch=24, lrate=0.010, error=2345.602\n",
      ">epoch=25, lrate=0.010, error=2339.296\n",
      ">epoch=26, lrate=0.010, error=2333.332\n",
      ">epoch=27, lrate=0.010, error=2327.689\n",
      ">epoch=28, lrate=0.010, error=2322.343\n",
      ">epoch=29, lrate=0.010, error=2317.277\n",
      ">epoch=30, lrate=0.010, error=2312.472\n",
      ">epoch=31, lrate=0.010, error=2307.913\n",
      ">epoch=32, lrate=0.010, error=2303.583\n",
      ">epoch=33, lrate=0.010, error=2299.470\n",
      ">epoch=34, lrate=0.010, error=2295.559\n",
      ">epoch=35, lrate=0.010, error=2291.840\n",
      ">epoch=36, lrate=0.010, error=2288.301\n",
      ">epoch=37, lrate=0.010, error=2284.932\n",
      ">epoch=38, lrate=0.010, error=2281.723\n",
      ">epoch=39, lrate=0.010, error=2278.665\n",
      ">epoch=40, lrate=0.010, error=2275.749\n",
      ">epoch=41, lrate=0.010, error=2272.969\n",
      ">epoch=42, lrate=0.010, error=2270.316\n",
      ">epoch=43, lrate=0.010, error=2267.784\n",
      ">epoch=44, lrate=0.010, error=2265.366\n",
      ">epoch=45, lrate=0.010, error=2263.056\n",
      ">epoch=46, lrate=0.010, error=2260.849\n",
      ">epoch=47, lrate=0.010, error=2258.739\n",
      ">epoch=48, lrate=0.010, error=2256.722\n",
      ">epoch=49, lrate=0.010, error=2254.792\n",
      ">epoch=50, lrate=0.010, error=2252.945\n",
      ">epoch=1, lrate=0.010, error=13024.990\n",
      ">epoch=2, lrate=0.010, error=2831.997\n",
      ">epoch=3, lrate=0.010, error=2770.652\n",
      ">epoch=4, lrate=0.010, error=2721.066\n",
      ">epoch=5, lrate=0.010, error=2680.092\n",
      ">epoch=6, lrate=0.010, error=2645.587\n",
      ">epoch=7, lrate=0.010, error=2616.014\n",
      ">epoch=8, lrate=0.010, error=2590.263\n",
      ">epoch=9, lrate=0.010, error=2567.526\n",
      ">epoch=10, lrate=0.010, error=2547.211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=11, lrate=0.010, error=2528.876\n",
      ">epoch=12, lrate=0.010, error=2512.192\n",
      ">epoch=13, lrate=0.010, error=2496.906\n",
      ">epoch=14, lrate=0.010, error=2482.823\n",
      ">epoch=15, lrate=0.010, error=2469.789\n",
      ">epoch=16, lrate=0.010, error=2457.682\n",
      ">epoch=17, lrate=0.010, error=2446.401\n",
      ">epoch=18, lrate=0.010, error=2435.863\n",
      ">epoch=19, lrate=0.010, error=2425.998\n",
      ">epoch=20, lrate=0.010, error=2416.747\n",
      ">epoch=21, lrate=0.010, error=2408.058\n",
      ">epoch=22, lrate=0.010, error=2399.886\n",
      ">epoch=23, lrate=0.010, error=2392.191\n",
      ">epoch=24, lrate=0.010, error=2384.938\n",
      ">epoch=25, lrate=0.010, error=2378.094\n",
      ">epoch=26, lrate=0.010, error=2371.631\n",
      ">epoch=27, lrate=0.010, error=2365.522\n",
      ">epoch=28, lrate=0.010, error=2359.745\n",
      ">epoch=29, lrate=0.010, error=2354.276\n",
      ">epoch=30, lrate=0.010, error=2349.095\n",
      ">epoch=31, lrate=0.010, error=2344.186\n",
      ">epoch=32, lrate=0.010, error=2339.529\n",
      ">epoch=33, lrate=0.010, error=2335.111\n",
      ">epoch=34, lrate=0.010, error=2330.915\n",
      ">epoch=35, lrate=0.010, error=2326.930\n",
      ">epoch=36, lrate=0.010, error=2323.141\n",
      ">epoch=37, lrate=0.010, error=2319.539\n",
      ">epoch=38, lrate=0.010, error=2316.111\n",
      ">epoch=39, lrate=0.010, error=2312.848\n",
      ">epoch=40, lrate=0.010, error=2309.741\n",
      ">epoch=41, lrate=0.010, error=2306.780\n",
      ">epoch=42, lrate=0.010, error=2303.959\n",
      ">epoch=43, lrate=0.010, error=2301.268\n",
      ">epoch=44, lrate=0.010, error=2298.701\n",
      ">epoch=45, lrate=0.010, error=2296.251\n",
      ">epoch=46, lrate=0.010, error=2293.912\n",
      ">epoch=47, lrate=0.010, error=2291.678\n",
      ">epoch=48, lrate=0.010, error=2289.544\n",
      ">epoch=49, lrate=0.010, error=2287.505\n",
      ">epoch=50, lrate=0.010, error=2285.555\n",
      ">epoch=1, lrate=0.010, error=12999.344\n",
      ">epoch=2, lrate=0.010, error=2800.334\n",
      ">epoch=3, lrate=0.010, error=2737.934\n",
      ">epoch=4, lrate=0.010, error=2687.765\n",
      ">epoch=5, lrate=0.010, error=2646.519\n",
      ">epoch=6, lrate=0.010, error=2611.947\n",
      ">epoch=7, lrate=0.010, error=2582.438\n",
      ">epoch=8, lrate=0.010, error=2556.837\n",
      ">epoch=9, lrate=0.010, error=2534.304\n",
      ">epoch=10, lrate=0.010, error=2514.226\n",
      ">epoch=11, lrate=0.010, error=2496.149\n",
      ">epoch=12, lrate=0.010, error=2479.734\n",
      ">epoch=13, lrate=0.010, error=2464.722\n",
      ">epoch=14, lrate=0.010, error=2450.915\n",
      ">epoch=15, lrate=0.010, error=2438.155\n",
      ">epoch=16, lrate=0.010, error=2426.319\n",
      ">epoch=17, lrate=0.010, error=2415.304\n",
      ">epoch=18, lrate=0.010, error=2405.027\n",
      ">epoch=19, lrate=0.010, error=2395.418\n",
      ">epoch=20, lrate=0.010, error=2386.416\n",
      ">epoch=21, lrate=0.010, error=2377.969\n",
      ">epoch=22, lrate=0.010, error=2370.033\n",
      ">epoch=23, lrate=0.010, error=2362.567\n",
      ">epoch=24, lrate=0.010, error=2355.536\n",
      ">epoch=25, lrate=0.010, error=2348.907\n",
      ">epoch=26, lrate=0.010, error=2342.653\n",
      ">epoch=27, lrate=0.010, error=2336.746\n",
      ">epoch=28, lrate=0.010, error=2331.163\n",
      ">epoch=29, lrate=0.010, error=2325.882\n",
      ">epoch=30, lrate=0.010, error=2320.884\n",
      ">epoch=31, lrate=0.010, error=2316.149\n",
      ">epoch=32, lrate=0.010, error=2311.662\n",
      ">epoch=33, lrate=0.010, error=2307.407\n",
      ">epoch=34, lrate=0.010, error=2303.369\n",
      ">epoch=35, lrate=0.010, error=2299.535\n",
      ">epoch=36, lrate=0.010, error=2295.892\n",
      ">epoch=37, lrate=0.010, error=2292.430\n",
      ">epoch=38, lrate=0.010, error=2289.137\n",
      ">epoch=39, lrate=0.010, error=2286.004\n",
      ">epoch=40, lrate=0.010, error=2283.022\n",
      ">epoch=41, lrate=0.010, error=2280.181\n",
      ">epoch=42, lrate=0.010, error=2277.475\n",
      ">epoch=43, lrate=0.010, error=2274.895\n",
      ">epoch=44, lrate=0.010, error=2272.434\n",
      ">epoch=45, lrate=0.010, error=2270.087\n",
      ">epoch=46, lrate=0.010, error=2267.846\n",
      ">epoch=47, lrate=0.010, error=2265.706\n",
      ">epoch=48, lrate=0.010, error=2263.662\n",
      ">epoch=49, lrate=0.010, error=2261.709\n",
      ">epoch=50, lrate=0.010, error=2259.842\n",
      ">epoch=1, lrate=0.010, error=13037.881\n",
      ">epoch=2, lrate=0.010, error=2841.101\n",
      ">epoch=3, lrate=0.010, error=2779.724\n",
      ">epoch=4, lrate=0.010, error=2730.395\n",
      ">epoch=5, lrate=0.010, error=2689.805\n",
      ">epoch=6, lrate=0.010, error=2655.744\n",
      ">epoch=7, lrate=0.010, error=2626.634\n",
      ">epoch=8, lrate=0.010, error=2601.341\n",
      ">epoch=9, lrate=0.010, error=2579.042\n",
      ">epoch=10, lrate=0.010, error=2559.139\n",
      ">epoch=11, lrate=0.010, error=2541.186\n",
      ">epoch=12, lrate=0.010, error=2524.852\n",
      ">epoch=13, lrate=0.010, error=2509.886\n",
      ">epoch=14, lrate=0.010, error=2496.093\n",
      ">epoch=15, lrate=0.010, error=2483.321\n",
      ">epoch=16, lrate=0.010, error=2471.450\n",
      ">epoch=17, lrate=0.010, error=2460.381\n",
      ">epoch=18, lrate=0.010, error=2450.033\n",
      ">epoch=19, lrate=0.010, error=2440.338\n",
      ">epoch=20, lrate=0.010, error=2431.240\n",
      ">epoch=21, lrate=0.010, error=2422.687\n",
      ">epoch=22, lrate=0.010, error=2414.636\n",
      ">epoch=23, lrate=0.010, error=2407.049\n",
      ">epoch=24, lrate=0.010, error=2399.891\n",
      ">epoch=25, lrate=0.010, error=2393.132\n",
      ">epoch=26, lrate=0.010, error=2386.744\n",
      ">epoch=27, lrate=0.010, error=2380.703\n",
      ">epoch=28, lrate=0.010, error=2374.984\n",
      ">epoch=29, lrate=0.010, error=2369.567\n",
      ">epoch=30, lrate=0.010, error=2364.433\n",
      ">epoch=31, lrate=0.010, error=2359.563\n",
      ">epoch=32, lrate=0.010, error=2354.943\n",
      ">epoch=33, lrate=0.010, error=2350.556\n",
      ">epoch=34, lrate=0.010, error=2346.388\n",
      ">epoch=35, lrate=0.010, error=2342.427\n",
      ">epoch=36, lrate=0.010, error=2338.661\n",
      ">epoch=37, lrate=0.010, error=2335.078\n",
      ">epoch=38, lrate=0.010, error=2331.667\n",
      ">epoch=39, lrate=0.010, error=2328.419\n",
      ">epoch=40, lrate=0.010, error=2325.326\n",
      ">epoch=41, lrate=0.010, error=2322.377\n",
      ">epoch=42, lrate=0.010, error=2319.567\n",
      ">epoch=43, lrate=0.010, error=2316.886\n",
      ">epoch=44, lrate=0.010, error=2314.328\n",
      ">epoch=45, lrate=0.010, error=2311.886\n",
      ">epoch=46, lrate=0.010, error=2309.555\n",
      ">epoch=47, lrate=0.010, error=2307.328\n",
      ">epoch=48, lrate=0.010, error=2305.201\n",
      ">epoch=49, lrate=0.010, error=2303.168\n",
      ">epoch=50, lrate=0.010, error=2301.224\n",
      "Scores: [0.7426356409988408, 0.7892702834662327, 0.7600576605337189, 0.7807954394481296, 0.7513764221555302]\n",
      "Mean RMSE: 0.765\n"
     ]
    }
   ],
   "source": [
    "# Contrived dataset for testing\n",
    "dataset = [[1, 1, 2], [2, 3, 1], [4, 3, 4], [3, 2, 2], [5, 5, 4]]  # x, y (actual)\n",
    "\n",
    "# Test prediction with provided coefficients\n",
    "coef = [0.4, 0.8, 0.6]   # y = 0.4 + 0.8x\n",
    "for row in dataset:\n",
    "    yhat = predict_linear(row, coef)\n",
    "    print(\"Expected=%.1f, Predicted=%.1f\" % (row[-1], yhat))\n",
    "\n",
    "# Test determining coefficients using stochastic gradient descent\n",
    "print()\n",
    "# Higher learning rate means greater leaps, but less able to fine-tune\n",
    "l_rate = 0.001\n",
    "# If learning rate is lower, we need more epochs to achieve higher accuracy\n",
    "n_epoch = 100\n",
    "coef = coefficients_linear_sgd(dataset, l_rate, n_epoch, 1)\n",
    "print(\"\\nIntercept:    \", coef[0])\n",
    "print(\"Coefficient 1:\", coef[1])\n",
    "print(\"Coefficient 2:\", coef[2])\n",
    "\n",
    "# Linear Regression With Stochastic Gradient Descent for Wine Quality\n",
    "seed(1)\n",
    "print(\"\\nLinear Regression With Stochastic Gradient Descent for Wine Quality:\")\n",
    "\n",
    "# Load and convert data to float\n",
    "filename = 'data/winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# Normalize, i.e. scale each value to between 0 and 1\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# Evaluate linear regression with SGD using dynamic test harness\n",
    "# 4,898 / 5 = 3916 rows for training and 979 for testing per fold, times five folds\n",
    "n_folds = 5\n",
    "l_rate = 0.01\n",
    "n_epoch = 50\n",
    "batch_size = 10\n",
    "# dataset, algorithm, n_folds, split, metric, *args (used to pass l_rate and n_epoch)\n",
    "scores = evaluate_algorithm(dataset, linear_regression_sgd, 5, rmse_metric, l_rate, n_epoch, batch_size)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
