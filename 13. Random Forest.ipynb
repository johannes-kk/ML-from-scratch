{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed, randrange\n",
    "from math import sqrt\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best Random Forest splits for a dataset (subsample), with constrained feature candidates\n",
    "# Input: dataset (subsample, like bagging), number of input features to (randomly) evaluate as splits\n",
    "# Output: dictionary of optimal split point with feature index, split value,\n",
    "def rf_get_split(dataset, n_features):\n",
    "    # Get unique output values by looking through last dataset column\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    # Initialize best split feature index, split value, gini score, resulting split groups of records\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    # Randomly select feature indices to test as splits, and add to list of candidates\n",
    "        # I.e. same as creating subsamples for bagging, but for input features\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        # Without replacement, e.g. if 8 features but n_features =4, get 4 random but unique indices\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    # Iterate over each randomly selected candidate feature\n",
    "    for index in features:\n",
    "        # Iterate over all rows in dataset, to try every possible feature value as split value\n",
    "        for row in dataset:\n",
    "            # Split entire dataset (subsample) using current feature and current feature value as split\n",
    "            groups = cart_test_split(index, row[index], dataset)\n",
    "            # Calculate Gini index of resulting groups for given class values\n",
    "            gini = cart_gini_index(groups, class_values)\n",
    "            # If gini of this feature + value as split is better, update dictionary of best parameters\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    # Return dictionary with best feature and value to split on, and resulting groups of dataset records\n",
    "        # Difference from CART is we didn't evaluate every input feature, only a random subset\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Random Forest recursively create child splits for a node, or make a terminal node\n",
    "# Input: pre-split node, max tree depth, min rows per node, node's depth, num features to evaluate\n",
    "# Output: None, i.e. void function\n",
    "def rf_split(node, max_depth, min_size, depth, n_features):\n",
    "    # Extract left and right lists of group rows from the supplied node (dictionary)\n",
    "    left, right = node['groups']\n",
    "    # Deletes groups of data from parent node, as it no longer needs access to the data\n",
    "    del(node['groups'])\n",
    "    # Checks whether left or right list empty, i.e. whether a no split (100% in one group)\n",
    "    if not left or not right:\n",
    "        # Make the only child a terminal node , and set 'left' and 'right' to point to it\n",
    "        node['left'] = node['right'] = cart_to_terminal(left + right)\n",
    "        # Exit current iteration, since terminal child node has no child nodes of its own\n",
    "        return\n",
    "    # Check whether supplied node is at or above maximum tree depth\n",
    "    if depth >= max_depth:\n",
    "        # Set left and right child nodes to terminal nodes\n",
    "        node['left'], node['right'] = cart_to_terminal(left), cart_to_terminal(right)\n",
    "        # Exit current iteration, i.e. halting progression down this branch\n",
    "        return\n",
    "\n",
    "    # If we reach this point, we neither have a no split, nor have reached max depth\n",
    "    # Process left child: if shorter than minimum row size, make it terminal\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = cart_to_terminal(left)\n",
    "    # Neither too deep nor too small, so split left child node to two child nodes\n",
    "    else:\n",
    "        # Split left child node\n",
    "        node['left'] = rf_get_split(left, n_features)\n",
    "        # Recursively call function on the split left child node in a depth first fashion\n",
    "        rf_split(node['left'], max_depth, min_size, depth+1, n_features)\n",
    "\n",
    "    # Process right child: if shorter than minimum, make it a terminal node\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = cart_to_terminal(right)\n",
    "    # If not, split right child node and make a recursive function call\n",
    "    else:\n",
    "        # Split right child node\n",
    "        node['right'] = rf_get_split(right, n_features)\n",
    "        # Make recursive call on the split right child\n",
    "        rf_split(node['right'], max_depth, min_size, depth+1, n_features)\n",
    "\n",
    "# Build a Random Forest decision tree\n",
    "def rf_build_tree(train, max_depth, min_size, n_features):\n",
    "    # Split the root node into two child nodes\n",
    "    root = rf_get_split(train, n_features)\n",
    "    # Call recursive function to add left nodes then right nodes in a depth first fashion\n",
    "    rf_split(root, max_depth, min_size, 1, n_features)\n",
    "    # Return root node; now just a dictionary with two child node references\n",
    "    # Similarly, its child nodes are only references, until terminal nodes which contain rows\n",
    "    return root\n",
    "\n",
    "# Random Forest Algorithm\n",
    "# Input: train test sets, tree max depth, min rows per node, subsample ratio, num trees, num features\n",
    "# Output: list of predictions corresponding to test set\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features = \"default\"):\n",
    "    # If using default number of random features to evaluate, use square root of total num features\n",
    "    if(n_features == \"default\"):\n",
    "        n_features = int(sqrt(len(dataset[0])-1))\n",
    "    # Create list to hold the tree trained on each subsample\n",
    "    trees = list()\n",
    "    # For each tree, bootstrap a subsample, train the random forest tree on it, and append to list\n",
    "    for i in range(n_trees):\n",
    "        sample = bootstrap_subsample(train, sample_size)\n",
    "        tree = rf_build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    # For each row in test, get prediction as most common random forest tree prediction\n",
    "    predictions = [bagging_predict_mode(trees, row) for row in test]\n",
    "    # Returns list of predictions for each row in test dataset\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RF on Sonar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sonar Case Study:\n",
      "Loaded data file data/sonar.all-data.csv with 208 rows and 61 columns.\n",
      "\n",
      "Trees: 1\n",
      "Scores: [51.21951219512195, 78.04878048780488, 58.536585365853654, 65.85365853658537, 53.65853658536586]\n",
      "Mean Accuracy: 61.463%\n",
      "\n",
      "Trees: 5\n",
      "Scores: [63.41463414634146, 60.97560975609756, 56.09756097560976, 60.97560975609756, 56.09756097560976]\n",
      "Mean Accuracy: 59.512%\n",
      "\n",
      "Trees: 10\n",
      "Scores: [65.85365853658537, 58.536585365853654, 68.29268292682927, 53.65853658536586, 75.60975609756098]\n",
      "Mean Accuracy: 64.390%\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"\\nSonar Case Study:\")\n",
    "dataset = load_csv('data/sonar.all-data.csv')\n",
    "# Convert string attributes to floats\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "# Convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# Evaluate algorithm\n",
    "# 208/5 ~ 41 records evaluated upon each iteration => 41 test, 167 train => 5 times\n",
    "n_folds = 5\n",
    "\n",
    "# Relatively deep trees are permitted, with relatively narrow branches (few records) allowed\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "# Calculating number of features for random forest to evaluate\n",
    "    # Algorithm has this built-in by default, but included here for clarity\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "\n",
    "# Run the algorithm for different numbers of samples (hence bagged trees, since one per sample)\n",
    "    # Primarily to demonstrate the behaviour of the algorithm\n",
    "for n_trees in [1, 5, 10]:\n",
    "    scores = evaluate_algorithm(dataset, random_forest, n_folds, accuracy_metric, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "\n",
    "    print('\\nTrees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
