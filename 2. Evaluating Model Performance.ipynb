{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from prettytable import PrettyTable\n",
    "from random import randrange, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "* Train/test dataset split\n",
    "* K-fold cross-validation dataset split\n",
    "* Classification accuracy and confusion matrix\n",
    "* Regression MAE and RMSE\n",
    "* Random selection and Zero rule baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into a train and test set, default 60/40 split\n",
    "def train_test_split(dataset, split=0.60):\n",
    "    train = list()\n",
    "    # Number of rows training set requires from original dataset\n",
    "    train_size = split * len(dataset)\n",
    "    # Python passes by reference, so would otherwise change original dataset\n",
    "    dataset_copy = list(dataset)\n",
    "    # continue taking random elements until training set is defined length\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        # Pop returns one element (row, here) and removes it from the object\n",
    "        # Takes a random row from dataset_copy and removes it from the pool\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    # returns training set (0.6) and remainder of dataset_copy, i.e. testing set (0.4)\n",
    "    return train, dataset_copy\n",
    "\n",
    "# Instead of two, divide into k groups (folds) of data of equal size.\n",
    "# For each group k, train the algorithm on the remaining k-1 groups, and test on k.\n",
    "# Split dataset into k folds (3 by default)\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    # List of folds, i.e. each list object is a fold\n",
    "    dataset_split = list()\n",
    "    # Operate on a copy since dataset passed by reference, keeps original intact\n",
    "    dataset_copy = list(dataset)\n",
    "    # int division trims off excess rows that keep it from dividing into equal chunks\n",
    "    fold_size = int(len(dataset)/folds)\n",
    "    # Iterating over 0 to folds\n",
    "    for i in range(folds):\n",
    "        # Create list to hold rows in fold i\n",
    "        fold = list()\n",
    "        # Populate fold with rows until requisite size (determined by k and dataset)\n",
    "        while len(fold) < fold_size:\n",
    "            # Pick random index of the dataset row to add to fold i\n",
    "            index = randrange(len(dataset_copy))\n",
    "            # Add row to fold and remove from pool of possible subsequent rows\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        # Add populated fold to list of fold objects\n",
    "        dataset_split.append(fold)\n",
    "    # Return list of folds; each is itself a list of rows\n",
    "    return dataset_split\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100\n",
    "\n",
    "# Construct confusion matrix\n",
    "# Returns two objects: the set of unique actual values, and the matrix\n",
    "# Matrix: 1st is actual values, 2nd is corresponding predictions values\n",
    "def confusion_matrix(actual, predicted):\n",
    "    # The \"Set\" (class, similar to list) of unique values in actual\n",
    "    unique = set(actual)\n",
    "    # matrix is a list of lists; one for each unique actual value\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    # Default each cell to 0, then change later.\n",
    "    for i in range(len(unique)):\n",
    "        # Confusion matrix is always square\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "    # Dictionary to index unique actual values\n",
    "    lookup = dict()\n",
    "    # Enumerate set of unique actuals\n",
    "    for i, value in enumerate(unique):\n",
    "        # Assign each unique actual value an index i (from enumeration)\n",
    "        # Uses \"value\" as dict key, index i as dict's value\n",
    "        lookup[value] = i\n",
    "    # Iterate over all actual/prediction pairs\n",
    "    for i in range(len(actual)):\n",
    "        # Get actual value's index (i) from dictionary\n",
    "        x = lookup[actual[i]]\n",
    "        # Get predicted value's index (i) from dictionary\n",
    "        y = lookup[predicted[i]]\n",
    "        # Increment matrix cell count at index_1 = actual, index_2 = predicted\n",
    "        matrix[x][y] += 1\n",
    "    # Returns the set of unique values, and the matrix itself\n",
    "    return unique, matrix\n",
    "\n",
    "# Print human-readable confusion matrix, using PrettyTable\n",
    "def print_confusion_matrix(unique, matrix):\n",
    "    table = PrettyTable()\n",
    "    # Set table headers\n",
    "    table.field_names = [\"A\\P\"] + [str(x) for x in unique]\n",
    "    # Matrix: iterate over unique actual values -> for each, get counts of unique prediction values\n",
    "    for i, value in enumerate(unique):\n",
    "        # Matrix[i][j] = count of prediction j for actual i, e.g. nrs. of \"No\" and \"Yes\" when actual is \"Yes\"\n",
    "        row = [str(value)] + [str(count) for count in matrix[i]]\n",
    "        table.add_row(row)\n",
    "    print(table)\n",
    "    \n",
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    # iterate over all the values\n",
    "    for i in range(len(actual)):\n",
    "        sum_error += abs(predicted[i] - actual[i])\n",
    "    # return MAE, float conversion to avoid integer division\n",
    "    return sum_error / float(len(actual))\n",
    "\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        #prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += ((predicted[i] - actual[i])**2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    "\n",
    "# Generate random predictions\n",
    "def random_algorithm(train, test):\n",
    "    # Store output values in training set. Assumes the final column [-1] is the output\n",
    "    output_values = [row[-1] for row in train]\n",
    "    # Set-object's constructor gets unique values, and is then converted to a list\n",
    "    unique = list(set(output_values))\n",
    "    # List of algo prediction, index in list = test dataset row number\n",
    "    predicted = list()\n",
    "    # For each row in test set, select random output value as prediction\n",
    "    for row in test:\n",
    "        # Picks a random output value: probability independent of distribution!!!\n",
    "        index = randrange(len(unique))\n",
    "        # Set the randomly selected output value as the prediction for that row\n",
    "        predicted.append(unique[index])\n",
    "    return predicted\n",
    "\n",
    "# Zero Rule for Classification models: for each testing row, predict the most common training output\n",
    "def zero_rule_algorithm_classification(train, test):\n",
    "    # Assuming output is final column in dataset, retrieve all outputs in training set\n",
    "    output_values = [row[-1] for row in train]\n",
    "    # Find most common (max of counts of each distinct value) output value in training set\n",
    "    # If multiple predictions share the max count, the first observed in the set is returned\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    # Predict the same value (most common in training set) for every row in testing set\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted\n",
    "\n",
    "# Zero Rule for Regression: for each testing row, predict the mean of training outputs\n",
    "def zero_rule_algorithm_regression(train, test):\n",
    "    # Assuming output is final column in dataset\n",
    "    output_values = [row[-1] for row in train]\n",
    "    # Calculates mean, used as our prediction\n",
    "    prediction = sum(output_values) / float(len(output_values))\n",
    "    # Predict the same value (mean of training outputs) for every row in testing set\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing train/test and cross validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Train and Test Split\n",
      "Train (60%): [[3], [2], [7], [1], [8], [9]]\n",
      "Test  (40%): [[4], [5], [6], [10]] \n",
      "\n",
      "> k-fold Cross Validation Split\n",
      "4 folds: [[[3], [2]], [[7], [1]], [[8], [9]], [[10], [6]]]\n"
     ]
    }
   ],
   "source": [
    "# test train/test split\n",
    "seed(1) # ensure exact same split of data every time code is executed\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "# Declare two lists simultaneously, since method returns two lists\n",
    "train, test = train_test_split(dataset)\n",
    "print(\"> Train and Test Split\")\n",
    "print(\"Train (60%):\", train)\n",
    "print(\"Test  (40%):\", test, \"\\n\")\n",
    "\n",
    "# Test cross validation split\n",
    "seed(1)\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "# Dataset of 10 \"rows\" divided into 4 folds means each is of size 2\n",
    "folds = cross_validation_split(dataset, 4)\n",
    "print(\"> k-fold Cross Validation Split\")\n",
    "print(\"4 folds:\", folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: 80.0 % \n",
      "\n",
      "Confusion matrix:\n",
      "+-----+---+---+\n",
      "| A\\P | 0 | 1 |\n",
      "+-----+---+---+\n",
      "|  0  | 3 | 2 |\n",
      "|  1  | 1 | 4 |\n",
      "+-----+---+---+\n",
      "+-----+-----+----+\n",
      "| A\\P | Yes | No |\n",
      "+-----+-----+----+\n",
      "| Yes |  4  | 2  |\n",
      "|  No |  1  | 3  |\n",
      "+-----+-----+----+\n",
      "\n",
      "MAE:  0.007999999999999993\n",
      "RMSE: 0.00894427190999915\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy metric\n",
    "actual = [0,0,0,0,0,1,1,1,1,1]  # 10 data points\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]   # 2 mistakes\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(\"Classification:\", accuracy, \"% \\n\")\n",
    "\n",
    "# Test confusion matrix\n",
    "print(\"Confusion matrix:\")\n",
    "actual = [0,0,0,0,0,1,1,1,1,1]  # 10 data points\n",
    "predicted = [0,1,1,0,0,1,0,1,1,1]   # 3 mistakes\n",
    "\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print_confusion_matrix(unique, matrix)\n",
    "\n",
    "actual2 = [\"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\"]\n",
    "predicted2 = [\"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "unique2, matrix2 = confusion_matrix(actual2, predicted2)\n",
    "print_confusion_matrix(unique2, matrix2)\n",
    "\n",
    "# Test MAE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "# all except one are wrong by 0.01; expect MAE just < 0.01\n",
    "mae = mae_metric(actual, predicted)\n",
    "print(\"\\nMAE: \", mae)\n",
    "\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing simple baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random predictions: [0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "\n",
      "Zero Rule classification predictions: ['0', '0', '0', '0']\n",
      "Zero Rule regression predictions: [15.0, 15.0, 15.0, 15.0]\n"
     ]
    }
   ],
   "source": [
    "# Testing random prediction generator\n",
    "seed(1)\n",
    "train = [[0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1]]\n",
    "test = [[None], [None], [None], [None], [None], [None], [None], [None], [None], [None], [None]]\n",
    "predictions = random_algorithm(train, test)\n",
    "print(\"Random predictions:\",predictions)\n",
    "\n",
    "# Testing zero rule for classification\n",
    "train = [['0'], ['0'], ['0'], ['0'], ['1'], ['1']]\n",
    "test = [[None], [None], [None], [None]]\n",
    "predictions = zero_rule_algorithm_classification(train, test)\n",
    "print(\"\\nZero Rule classification predictions:\", predictions)\n",
    "\n",
    "# Testing zero rule for regression\n",
    "train = [[10], [15], [12], [15], [18], [20]]\n",
    "test = [[None], [None], [None], [None]]\n",
    "predictions = zero_rule_algorithm_regression(train, test)\n",
    "print(\"Zero Rule regression predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
